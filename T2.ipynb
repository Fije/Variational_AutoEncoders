{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1 - T2\n",
    "\n",
    "Neural IBM 1 with additional French context (Section 2.1 of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from utils import smart_reader, bitext_reader\n",
    "from vocabulary import OrderedCounter, Vocabulary \n",
    "from utils import iterate_minibatches, prepare_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'\n",
    "test_e_path = 'data/test/test.e.gz'\n",
    "test_f_path = 'data/test/test.f.gz'\n",
    "test_wa = 'data/test/test.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the vocabularies that we use further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model\n",
    "\n",
    "We import the NeuralIBM1Model from T2.py and NeuralIBM1Trainer from T2trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from T2 import NeuralIBM1Model\n",
    "from T2trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with B=10 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss 52.491070 accuracy 0.26 lr 0.001000\n",
      "Iter   200 loss 77.566437 accuracy 0.14 lr 0.001000\n",
      "Iter   300 loss 59.488373 accuracy 0.29 lr 0.001000\n",
      "Iter   400 loss 48.343189 accuracy 0.20 lr 0.001000\n",
      "Iter   500 loss 49.720253 accuracy 0.15 lr 0.001000\n",
      "Iter   600 loss 43.222996 accuracy 0.22 lr 0.001000\n",
      "Iter   700 loss 36.280529 accuracy 0.30 lr 0.001000\n",
      "Epoch 1 loss 62.710171 accuracy 0.20 val_aer 0.69 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 82.185562 accuracy 0.17 lr 0.001000\n",
      "Iter   200 loss 80.753525 accuracy 0.18 lr 0.001000\n",
      "Iter   300 loss 36.255074 accuracy 0.24 lr 0.001000\n",
      "Iter   400 loss 50.230843 accuracy 0.24 lr 0.001000\n",
      "Iter   500 loss 58.847137 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss 54.897614 accuracy 0.20 lr 0.001000\n",
      "Iter   700 loss 52.753399 accuracy 0.16 lr 0.001000\n",
      "Epoch 2 loss 50.650735 accuracy 0.21 val_aer 0.76 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 60.805134 accuracy 0.15 lr 0.001000\n",
      "Iter   200 loss 45.120152 accuracy 0.22 lr 0.001000\n",
      "Iter   300 loss 47.440811 accuracy 0.22 lr 0.001000\n",
      "Iter   400 loss 36.004215 accuracy 0.23 lr 0.001000\n",
      "Iter   500 loss 41.842133 accuracy 0.16 lr 0.001000\n",
      "Iter   600 loss 36.487080 accuracy 0.26 lr 0.001000\n",
      "Iter   700 loss 29.629917 accuracy 0.26 lr 0.001000\n",
      "Epoch 3 loss 42.730655 accuracy 0.21 val_aer 0.78 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 26.356182 accuracy 0.25 lr 0.001000\n",
      "Iter   200 loss 47.894875 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss 30.451145 accuracy 0.21 lr 0.001000\n",
      "Iter   400 loss 26.913845 accuracy 0.23 lr 0.001000\n",
      "Iter   500 loss 52.160107 accuracy 0.21 lr 0.001000\n",
      "Iter   600 loss 40.713036 accuracy 0.25 lr 0.001000\n",
      "Iter   700 loss 28.152441 accuracy 0.28 lr 0.001000\n",
      "Epoch 4 loss 40.196752 accuracy 0.21 val_aer 0.77 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 38.850060 accuracy 0.18 lr 0.001000\n",
      "Iter   200 loss 49.297935 accuracy 0.19 lr 0.001000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # some hyper-parameters\n",
    "    # tweak them as you wish\n",
    "    batch_size=10  # on CPU, use something much smaller e.g. 1-16\n",
    "    max_length=30\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "    emb_dim = 64\n",
    "    mlp_dim = 128\n",
    "\n",
    "    # our model\n",
    "    model = NeuralIBM1Model(\n",
    "        x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "        batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "\n",
    "    # our trainer\n",
    "    trainer = NeuralIBM1Trainer(\n",
    "        model, train_e_path, train_f_path, \n",
    "        dev_e_path, dev_f_path, dev_wa,\n",
    "        test_e_path, test_f_path, test_wa,\n",
    "        num_epochs=5, batch_size=batch_size, \n",
    "        max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess,\n",
    "        max_num=10000) # small training corpus just to make testing new code easier\n",
    "\n",
    "    # now first TF needs to initialize all the variables\n",
    "    print(\"Initializing variables..\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    # now we can start training!\n",
    "    print(\"Training started..\")\n",
    "    results = trainer.train()\n",
    "    dev_AERs, test_AERs, train_likelihoods, dev_likelihoods = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_AERs)+1), dev_AERs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "ax2 = plt.plot(range(1, len(test_AERs)+1), test_AERs, label='test-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AER')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(train_likelihoods)+1), train_likelihoods, label='training-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax2 = plt.plot(range(1, len(dev_likelihoods)+1), dev_likelihoods, label='dev-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
