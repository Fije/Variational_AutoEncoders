{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a VAE in TensorFlow\n",
    "\n",
    "\n",
    "Consider the following graphical model:\n",
    "\n",
    "![VAE](img/vae.png)\n",
    "\n",
    "where \n",
    "* $Z \\sim \\mathcal N(0, I)$ is a random embedding\n",
    "* $X \\sim \\mathrm{Cat}(f_\\theta(z))$ is a categorical distribution over a language's vocabulary\n",
    "* $f_\\theta(z) = \\mathrm{softmax(g_\\theta(z))}$ is a FFNN that predicts the parameters of our CPD\n",
    "\n",
    "Here is our joint distribution \n",
    "\\begin{align}\n",
    "p_\\theta(x, z) &= p_\\theta(z) P_\\theta(x|z) \\\\\n",
    " &= \\mathcal N(0, I) P_\\theta(x|z)\n",
    "\\end{align}\n",
    "\n",
    "Note that the marginal likelihood is intractable\n",
    "\n",
    "\\begin{align}\n",
    "    P_\\theta(x) &=  \\int p_\\theta(z,x) \\mathrm{d}z \\\\\n",
    "    &= p_\\theta(z)P_\\theta(x|z) \\\\\n",
    "\\end{align}\n",
    "\n",
    "because of the marginalisation over all possible random embeddings and this makes our posterior intractable too.\n",
    "\n",
    "## Training\n",
    "\n",
    "We will use variational inference to circumvent the intractable marginalisation, where we propose a variational approximation (a.k.a. *inference network*) $q_\\phi(z|x)$ with its own parameters $\\phi$.\n",
    "Since $Z$ is Gaussian-distributed, we choose $q_\\phi(z|x) = \\mathcal N(\\mu_\\phi(x), \\sigma^2_\\phi(x))$, where\n",
    "\n",
    "* $\\mu_\\phi(x) = u_\\phi(x)$\n",
    "* $\\sigma^2_\\phi(x) = \\exp(s_\\phi(x))$\n",
    "\n",
    "are FFNNs that locally predict an approximation to the true posterior mean and variance for each observation $x$.\n",
    "\n",
    "Our variational auto-encoder then boils down to:\n",
    "\n",
    "* an *inference network*, i.e., a neural network that \n",
    "    * reads in words\n",
    "    * embeds them\n",
    "    * for each word: \n",
    "        * predicts a vector of means $\\mu_\\phi(x)$\n",
    "        * predicts a vector of (log) variances $\\sigma_\\phi^2(x)$\n",
    "        * samples a random embedding by sampling $\\epsilon \\sim \\mathcal N(0, I)$ and returning $\\mu_\\phi(x) + \\epsilon \\sigma_\\phi(x)$\n",
    "\n",
    "* a *generative model*, i.e., a neural network that for each word position\n",
    "    * takes a sampled embedding $z$\n",
    "    * predicts the parameters of a categorical distribution over the vocabulary $f_\\theta(x)$\n",
    "    \n",
    "You will identify all these steps in the code.\n",
    "\n",
    "The model is trained to maximise a lowerbound on log-likelihood of training data, the ELBO:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E_{\\mathcal D}(\\theta, \\phi) &= \\frac{1}{|D|} \\sum_{x_1^n \\in \\mathcal D} \\underbrace{\\sum_{i=1}^{n} \\underbrace{\\mathcal E(\\theta, \\phi|x_i)}_{\\text{word}}}_{\\text{sentence}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal D$ is a set made of $|\\mathcal D|$ sentences, each of which is itself a sequence of words.\n",
    "The contribution to the ELBO due to each sentence is the sum of contributions from each word:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E(\\theta, \\phi|x) &= \\mathbb E_{q_\\phi(Z|x)} \\left[ \\log P_\\theta(x|Z) \\right] - \\mathrm{KL}(q_\\phi(Z|x)||p_\\theta(z)) \\\\\n",
    " &= \\mathbb E_{\\epsilon \\sim \\mathcal N(0,I)} \\left[ \\log P_\\theta(x|Z=\\mu_\\phi(x) + \\epsilon \\sigma_\\phi(x)) \\right] - \\mathrm{KL}(q_\\phi(Z|x)||\\mathcal N(0, I)) \n",
    "\\end{align}\n",
    "\n",
    "which we usually approximate with a single sample $\\epsilon \\sim N(0, I)$ for each word $x$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E(\\theta, \\phi|x) \n",
    " &\\approx \\mathbb \\log P_\\theta(x|Z=\\mu_\\phi(x) + \\epsilon \\sigma_\\phi(x)) - \\mathrm{KL}(q_\\phi(Z|x)||\\mathcal N(0, I)) \n",
    "\\end{align}\n",
    "\n",
    "and the KL term can be computed analytically\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{KL}(q(Z|x)||\\mathcal N(0, I)) &= -\\frac{1}{2} \\sum_{j=1}^d \\left( 1 + \\log \\sigma^2_{\\phi,j}(x) - \\mu^2_{\\phi,j}(x) - \\sigma^2_{\\phi,j}(x) \\right)\n",
    "\\end{align}\n",
    "\n",
    "where the summation is defined over the $d$ components of the mean and variance vectors.\n",
    "\n",
    "\n",
    "## Posterior Inference\n",
    "\n",
    "Note that in general, because the generative model involves non-linear functions of $Z$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb E_{p_\\theta(Z|X)}[ f(Z) ]  & \\neq f\\left(\\mathbb E_{p_\\theta(Z|X)}[Z] \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $p(Z|X=x)$ is approximated by our variational distribution $q_\\phi(Z|X=x)$.\n",
    "\n",
    "This means that decoding the mean is not the same as the mean decoding for a certain decoder $f$.\n",
    "\n",
    "Nonetheless, we will make a simplifying assumption here and approximate $\\mathbb E_{p_\\theta(Z|X=x)}[Z]$ by the predicted mean $\\mu_\\phi(x)$.\n",
    "\n",
    "A more principled approach would sample a few times from the approximate posterior and use a stochastic decoder (e.g. MBR), but this is beyond the scope of project 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter, OrderedDict\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first load some data\n",
    "\n",
    "We define a reader that returns one sentence at a time, without loading the whole data set into memory.\n",
    "This is done using the \"yield\" command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['36', 'th', 'Parliament', ',', '2', 'nd', 'Session']\n",
      "['edited', 'HANSARD', '*', 'NUMBER', '1']\n",
      "['contents']\n"
     ]
    }
   ],
   "source": [
    "from utils import smart_reader, filter_len\n",
    "\n",
    "\n",
    "def reader_test(path):\n",
    "  # corpus is now a generator that gives us a list of tokens (a sentence) \n",
    "  # everytime a function calls \"next\" on it\n",
    "  corpus = filter_len(smart_reader(train_en_path), max_length=10)\n",
    "\n",
    "  # to see that it really works, try this:\n",
    "  print(next(corpus))\n",
    "  print(next(corpus))\n",
    "  print(next(corpus))\n",
    "  \n",
    "  \n",
    "# the path to our training data, English side\n",
    "train_en_path = 'data/training/hansards.36.2.e.gz'\n",
    "\n",
    "# Let's try it:\n",
    "reader_test(train_en_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's create a vocabulary!\n",
    "\n",
    "We first define a class `Vocabulary` that helps us convert tokens (words) into numbers. This is useful later, because then we can e.g. index a word embedding table using the ID of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our Vocabulary class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 36640\n",
      "Trimmed vocabulary size: 10005\n",
      "The index of \"*PAD*\" is: 1\n",
      "The index of \"<UNK>\" is: 1\n",
      "The index of \"the\" is: 5\n",
      "The token with index 0 is: <PAD>\n",
      "The token with index 1 is: <UNK>\n",
      "The token with index 2 is: <S>\n",
      "The token with index 3 is: </S>\n",
      "The token with index 4 is: <NULL>\n",
      "The token with index 5 is: the\n",
      "The token with index 6 is: .\n",
      "The token with index 7 is: ,\n",
      "The token with index 8 is: of\n",
      "The token with index 9 is: to\n",
      "The index of \"!@!_not_in_vocab_!@!\" is: 1\n"
     ]
    }
   ],
   "source": [
    "# We used up a few lines in the previous example, so we set up\n",
    "# our data generator again.\n",
    "corpus = smart_reader(train_en_path)    \n",
    "\n",
    "# Let's create a vocabulary given our (tokenized) corpus\n",
    "vocabulary = Vocabulary(corpus=corpus)\n",
    "print(\"Original vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "# Now we only keep the highest-frequency words\n",
    "vocabulary_size=10000\n",
    "vocabulary.trim(vocabulary_size)\n",
    "print(\"Trimmed vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "# Now we can get word indexes using v.get_word_id():\n",
    "for t in [\"*PAD*\", \"<UNK>\", \"the\"]:\n",
    "  print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "\n",
    "# And the inverse too, using v.i2t:\n",
    "for i in range(10):\n",
    "  print(\"The token with index {} is: {}\".format(i, vocabulary.get_token(i)))\n",
    "\n",
    "# Now let's try to get a word ID for a word not in the vocabulary\n",
    "# we should get 1 (so, <UNK>)\n",
    "for t in [\"!@!_not_in_vocab_!@!\"]:\n",
    "  print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mini-batching\n",
    "\n",
    "With our vocabulary, we still need a method that converts a whole sentence to a sequence of IDs.\n",
    "And, to speed up training, we would like to get a so-called mini-batch at a time: multiple of such sequences together. So our function takes a corpus iterator and a vocabulary, and returns a mini-batch of dimension Batch X Time, where the first dimension indeces the sentences in the batch, and the second the time steps in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import iterate_minibatches, prepare_batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch of data that we will train on, as tokens:\n",
      "[['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'], ['edited', 'HANSARD', '*', 'NUMBER', '1'], ['contents'], ['Tuesday', ',', 'October', '12', ',', '1999']]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[   4 1203  745  325    7  262 2381 1963]\n",
      " [   4 2651 2665   67 2643  238    0    0]\n",
      " [   4 2873    0    0    0    0    0    0]\n",
      " [   4 1532    7  813  882    7  297    0]]\n",
      "\n",
      "Here is the original first sentence back again:\n",
      "['<NULL>', '36', 'th', 'Parliament', ',', '2', 'nd', 'Session']\n"
     ]
    }
   ],
   "source": [
    "# Let's try it out!\n",
    "corpus = smart_reader(train_en_path)          \n",
    "\n",
    "\n",
    "for batch_id, batch in enumerate(iterate_minibatches(corpus, batch_size=4)):\n",
    "\n",
    "  print(\"This is the batch of data that we will train on, as tokens:\")\n",
    "  print(batch)\n",
    "  print()\n",
    "\n",
    "  x = prepare_batch_data(batch, vocabulary)\n",
    "\n",
    "  print(\"These are our inputs (i.e. words replaced by IDs):\")\n",
    "  print(x)\n",
    "  print()\n",
    "  \n",
    "  print(\"Here is the original first sentence back again:\")\n",
    "  print([vocabulary.get_token(token_id) for token_id in x[0]])\n",
    "\n",
    "  break  # stop after the first batch, this is just a demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice the following:\n",
    "\n",
    "1. The longest sequence in the batch has no padding. Any sequences shorter, however, will have padding zeros.\n",
    "2. The length tensor gives the length for each sequence in the batch, so that we can correctly calculate the loss.\n",
    "\n",
    "With our input pipeline in place, now let's create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check vae.py to see the model\n",
    "from vae import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Now that we have a model, we need to train it. To do so we define a Trainer class that takes our model as an argument and trains it, keeping track of some important information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing variables..\n",
      "Training started..\n",
      "Iter 100 loss 546.1347045898438 ce 387.4946594238281 kl 158.64004516601562 acc 0.37 339/913 lr 0.001000\n"
     ]
    }
   ],
   "source": [
    "from vae_trainer import VAETrainer\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#   with tf.device(\"/cpu:0\"):   \n",
    "\n",
    "  batch_size=64\n",
    "  max_length=30\n",
    "\n",
    "  model = VAE(vocabulary=vocabulary, batch_size=batch_size, \n",
    "              emb_dim=64, rnn_dim=128, z_dim=64, context='gate')\n",
    "  trainer = VAETrainer(model, train_en_path, num_epochs=10, \n",
    "                  batch_size=batch_size, max_length=max_length,\n",
    "                  lr=0.001, lr_decay=0.0, session=sess)\n",
    "\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  print(\"Training started..\")\n",
    "  trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
