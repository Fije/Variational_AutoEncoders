{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4: Neural IBM1 with collocations and latent gate\n",
    "\n",
    "Neural IBM 1 with additional stochastic collocations (Section 2.3 of the notebook).\n",
    "\n",
    "Here we made the collocation variable $s$ continuous. You can interpret $S$ as a random variable over gate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from utils import smart_reader, bitext_reader\n",
    "from vocabulary import OrderedCounter, Vocabulary \n",
    "from utils import iterate_minibatches, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'\n",
    "test_e_path = 'data/test/test.e.gz'\n",
    "test_f_path = 'data/test/test.f.gz'\n",
    "test_wa = 'data/test/test.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the vocabularies that we use further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4 model\n",
    "\n",
    "We will import the `NeuralIBM1Model` (rewritten as a VAE) from `T4.py` and `NeuralIBM1Trainer` from `T4trainer.py`. But to show the code, we include it here (Have a look at the code. It is amazing. Really good code.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "from utils import iterate_minibatches, prepare_data\n",
    "import scipy as sp\n",
    "\n",
    "# for TF 1.1\n",
    "import tensorflow\n",
    "try:\n",
    "    from tensorflow.contrib.keras.initializers import glorot_uniform\n",
    "except:  # for TF 1.0\n",
    "    from tensorflow.contrib.layers import xavier_initializer as glorot_uniform\n",
    "\n",
    "class NeuralIBM1Model:\n",
    "    \"\"\"Our Neural IBM1 model.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=8,\n",
    "               x_vocabulary=None, y_vocabulary=None,\n",
    "               emb_dim=32, mlp_dim=64,\n",
    "               session=None):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.mlp_dim = mlp_dim\n",
    "\n",
    "        self.x_vocabulary = x_vocabulary\n",
    "        self.y_vocabulary = y_vocabulary\n",
    "        self.x_vocabulary_size = len(x_vocabulary)\n",
    "        self.y_vocabulary_size = len(y_vocabulary)\n",
    "\n",
    "        self._create_placeholders()\n",
    "        self._create_weights()\n",
    "        self._build_model()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.session = session\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        \"\"\"We define placeholders to feed the data to TensorFlow.\"\"\"\n",
    "        # \"None\" means the batches may have a variable maximum length.\n",
    "        self.x  = tf.placeholder(tf.int64, shape=[None, None],\n",
    "                                 name = \"english\")\n",
    "        self.yp = tf.placeholder(tf.int64, shape=[None, None],\n",
    "                                 name = \"prev_french\")\n",
    "        self.y  = tf.placeholder(tf.int64, shape=[None, None],\n",
    "                                 name = \"french\")\n",
    "\n",
    "    def _create_weights(self):\n",
    "        \"\"\"Create weights for the model.\"\"\"\n",
    "        # TIM: we need to double the input embedding size if the mode is concat.\n",
    "        emb_dim = self.emb_dim\n",
    "\n",
    "        with tf.variable_scope(\"MLP\") as scope:\n",
    "            # first layer for t\n",
    "            self.mlp_Wt_ = tf.get_variable(\n",
    "            name=\"Wt_\", initializer=glorot_uniform(),\n",
    "            shape=[emb_dim, self.mlp_dim])\n",
    "\n",
    "            self.mlp_bt_ = tf.get_variable(\n",
    "            name=\"bt_\", initializer=tf.zeros_initializer(),\n",
    "            shape=[self.mlp_dim])\n",
    "\n",
    "            # first layer for i and s\n",
    "            self.mlp_Wis_ = tf.get_variable(\n",
    "            name=\"Wis_\", initializer=glorot_uniform(),\n",
    "            shape=[emb_dim, self.mlp_dim])\n",
    "\n",
    "            self.mlp_bis_ = tf.get_variable(\n",
    "            name=\"bis_\", initializer=tf.zeros_initializer(),\n",
    "            shape=[self.mlp_dim])\n",
    "\n",
    "            # layer for translation P(F|E)\n",
    "            self.mlp_W_t = tf.get_variable(\n",
    "            name=\"W_t\", initializer=glorot_uniform(),\n",
    "            shape=[self.mlp_dim, self.y_vocabulary_size])\n",
    "\n",
    "            self.mlp_b_t = tf.get_variable(\n",
    "            name=\"b_t\", initializer=tf.zeros_initializer(),\n",
    "            shape=[self.y_vocabulary_size])\n",
    "\n",
    "            # layer for insertion P(F|Fprev)\n",
    "            self.mlp_W_i = tf.get_variable(\n",
    "            name=\"W_i\", initializer=glorot_uniform(),\n",
    "            shape=[self.mlp_dim, self.y_vocabulary_size])\n",
    "\n",
    "            self.mlp_b_i = tf.get_variable(\n",
    "            name=\"b_i\", initializer=tf.zeros_initializer(),\n",
    "            shape=[self.y_vocabulary_size])\n",
    "\n",
    "            # layer for laten gate S, P(S|Fprev)\n",
    "            self.a_W = tf.get_variable(\n",
    "            name=\"a_W\", initializer=tf.random_normal_initializer(),\n",
    "            shape=[self.mlp_dim, 1])\n",
    "\n",
    "            self.a_b = tf.get_variable(\n",
    "            name=\"a_b\", initializer=tf.random_normal_initializer(),\n",
    "            shape=[1])\n",
    "\n",
    "            self.b_W = tf.get_variable(\n",
    "            name=\"b_W\", initializer=tf.random_normal_initializer(),\n",
    "            shape=[self.mlp_dim, 1])\n",
    "\n",
    "            self.b_b = tf.get_variable(\n",
    "            name=\"b_b\", initializer=tf.random_normal_initializer(),\n",
    "            shape=[1])\n",
    "\n",
    "            self.alpha_W = tf.get_variable(\n",
    "            name=\"alpha_W\", initializer=tf.random_normal_initializer(),\n",
    "            shape=[self.mlp_dim, 1])\n",
    "\n",
    "            self.alpha_b = tf.get_variable(\n",
    "            name=\"alpha_b\", initializer=tf.random_normal_initializer(),\n",
    "            shape=[1])\n",
    "\n",
    "            self.beta_W = tf.get_variable(\n",
    "            name=\"beta_W\", initializer=tf.random_normal_initializer(),\n",
    "            shape=[self.mlp_dim, 1])\n",
    "\n",
    "            self.beta_b = tf.get_variable(\n",
    "            name=\"beta_b\", initializer=tf.random_normal_initializer(),\n",
    "            shape=[1])\n",
    "    \n",
    "    def save(self, session, path=\"model.ckpt\"):\n",
    "        \"\"\"Saves the model.\"\"\"\n",
    "        return self.saver.save(session, path)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Builds the computational graph for our model.\"\"\"\n",
    "\n",
    "        # 1. Let's create a (source) word embeddings matrix.\n",
    "        # These are trainable parameters, so we use tf.get_variable.\n",
    "        # Shape: [Vx, emb_dim] where Vx is the source vocabulary size\n",
    "        x_embeddings = tf.get_variable(\n",
    "          name=\"x_embeddings\", initializer=tf.random_uniform_initializer(),\n",
    "          shape=[self.x_vocabulary_size, self.emb_dim])\n",
    "        y_embeddings = tf.get_variable(\n",
    "          name=\"y_embeddings\", initializer=tf.random_uniform_initializer(),\n",
    "          shape=[self.y_vocabulary_size, self.emb_dim])\n",
    "\n",
    "        emb_dim = self.emb_dim\n",
    "\n",
    "        # Now we start defining our graph.\n",
    "\n",
    "        # ###############################################\n",
    "        # This is the **inference** network q_\\phi(Z | x)\n",
    "        #\n",
    "        #  it predicts for each x a d-dimensional vector of means and a vector of (log) variances\n",
    "        #  it does so from x's 1-hot encoding\n",
    "        #  thus the first step is to embed x\n",
    "\n",
    "        # first we need to know some sizes from the current input data\n",
    "        batch_size = tf.shape(self.x)[0]\n",
    "        longest_x = tf.shape(self.x)[1]  # longest M\n",
    "        longest_y = tf.shape(self.y)[1]  # longest N\n",
    "\n",
    "        # Input yp\n",
    "        twos = 2*tf.ones([tf.shape(self.y)[0], 1], tf.int64) # prepend all the sentences with '2', the code for <S>\n",
    "        yp = tf.concat([twos, self.y[:,:-1]], axis=1, name='concat-twos')\n",
    "        self.y_p = yp\n",
    "        yp_embedded = tf.nn.embedding_lookup(y_embeddings, yp) # Shape: [B, N, emb_dim]\n",
    "        self.yp_embedded = yp_embedded\n",
    "\n",
    "        # Input x\n",
    "        x_embedded = tf.nn.embedding_lookup(x_embeddings, self.x) # Shape: [B, M, emb_dim]\n",
    "\n",
    "\n",
    "        # It's also useful to have masks that indicate what\n",
    "        # values of our batch we should ignore.\n",
    "        # Masks have the same shape as our inputs, and contain\n",
    "        # 1.0 where there is a value, and 0.0 where there is padding.\n",
    "        x_mask  = tf.cast(tf.sign(self.x), tf.float32)    # Shape: [B, M]\n",
    "        y_mask  = tf.cast(tf.sign(self.y), tf.float32)    # Shape: [B, N]\n",
    "        x_len   = tf.reduce_sum(tf.sign(self.x), axis=1)  # Shape: [B]\n",
    "        y_len   = tf.reduce_sum(tf.sign(self.y), axis=1)  # Shape: [B]\n",
    "\n",
    "        # 2.a Build an alignment model P(A | X, M, N)\n",
    "\n",
    "        # This just gives you 1/length_x (already including NULL) per sample.\n",
    "        # i.e. the lengths are the same for each word y_1 .. y_N.\n",
    "        lengths  = tf.expand_dims(x_len, -1)  # Shape: [B, 1]\n",
    "        pa_x     = tf.div(x_mask, tf.cast(lengths, tf.float32))   # Shape: [B, M]\n",
    "\n",
    "        # We now have a matrix with 1/M values.\n",
    "        # For a batch of 2 setencnes, with lengths 2 and 3:\n",
    "        #\n",
    "        #  pa_x = [[1/2 1/2   0]\n",
    "        #          [1/3 1/3 1/3]]\n",
    "        #\n",
    "        # But later we will need it N times. So we repeat (=tile) this\n",
    "        # matrix N times, and for that we create a new dimension\n",
    "        # in between the current ones (dimension 1).\n",
    "        pa_x  = tf.expand_dims(pa_x, 1)  # Shape: [B, 1, M]\n",
    "\n",
    "        #  pa_x = [[[1/2 1/2   0]]\n",
    "        #          [[1/3 1/3 1/3]]]\n",
    "        # Note the extra brackets.\n",
    "\n",
    "        # Now we perform the tiling:\n",
    "        pa_x  = tf.tile(pa_x, [1, longest_y, 1])  # [B, N, M]\n",
    "\n",
    "        # pa_x = tf.tile(y_embedded, [1, 1, longest_y])\n",
    "\n",
    "        # Result:\n",
    "        #  pa_x = [[[1/2 1/2   0]\n",
    "        #           [1/2 1/2   0]]\n",
    "        #           [[1/3 1/3 1/3]\n",
    "        #           [1/3 1/3 1/3]]]\n",
    "\n",
    "        # The MLP\n",
    "\n",
    "        # This is for P(F|E), translation t\n",
    "        mlp_input = tf.reshape(x_embedded, [batch_size * longest_x, emb_dim], name='x-emb-reshape')\n",
    "        h_t = tf.matmul(mlp_input, self.mlp_Wt_, name='x1') + self.mlp_bt_ # Shape [B*M, emb_dim]\n",
    "        h_t  = tf.tanh(h_t)\n",
    "        h_t = tf.matmul(h_t, self.mlp_W_t, name='x2') + self.mlp_b_t # Shape [B*M, emb_dim]\n",
    "        # Now we perform a softmax which operates on a per-row basis.\n",
    "        py_xa = tf.nn.softmax(h_t)\n",
    "        # This is P(F|E)\n",
    "        py_xa = tf.reshape(py_xa, [batch_size, longest_x, self.y_vocabulary_size], name='py_xa-emb-reshape') # Shape [B, M, Vy]\n",
    "\n",
    "        # This is for P(F|Fprev) and P(C|Fprev), insertion i and collocation c\n",
    "        # Note: Shared first layer!\n",
    "        mlp_input = tf.reshape(yp_embedded, [batch_size * longest_y, emb_dim], name='yp-emb-reshape')\n",
    "        h_is = tf.matmul(mlp_input, self.mlp_Wis_, name='y1') + self.mlp_bis_ # Shape [B*N, emb_dim]\n",
    "        h_is  = tf.tanh(h_is)\n",
    "        # This is P(F|Fprev) insertion i\n",
    "        h_i = tf.matmul(h_is, self.mlp_W_i, name='y2') + self.mlp_b_i # Shape [B*N, emb_dim]\n",
    "        py_y  = tf.nn.softmax(h_i) # Shape: [B*N, Vy]\n",
    "        py_y = tf.reshape(py_y, [batch_size, longest_y, self.y_vocabulary_size], name='py_y-emb-reshape') # Shape [B, N, Vy]\n",
    "\n",
    "        # This for the prior p(Z) = Beta(a,b)\n",
    "        a = tf.matmul(h_is, self.a_W) + self.a_b  # [B*M, 1]\n",
    "        a = tf.exp(a)\n",
    "        a = tf.squeeze(a)\n",
    "        b = tf.matmul(h_is, self.b_W) + self.b_b  # [B*M, 1]\n",
    "        b = tf.exp(b)\n",
    "        b = tf.squeeze(b)\n",
    "        # This is for the approxiation of the posterior p(Z|X) using Kuma(alpha, beta)\n",
    "        alpha = tf.matmul(h_is, self.alpha_W) + self.alpha_b  # [B*N, 1]\n",
    "        alpha = tf.exp(alpha)\n",
    "        alpha = tf.squeeze(alpha)\n",
    "        self.alpha = tf.reshape(alpha, [batch_size, longest_y])\n",
    "        beta = tf.matmul(h_is, self.beta_W) + self.beta_b  # [B*N, 1]\n",
    "        beta = tf.exp(beta)\n",
    "        beta = tf.squeeze(beta)\n",
    "        self.beta = tf.reshape(beta, [batch_size, longest_y])\n",
    "\n",
    "\n",
    "\n",
    "        # ##############################################\n",
    "        # This is the *generative* network\n",
    "        #  it conditions on our sampled z to predict the parameters of a Categorical over the vocabulary\n",
    "\n",
    "\n",
    "        # #########################################################\n",
    "        # Prediction\n",
    "        #\n",
    "        # Note that while training is stochastic (we sample z by sampling epsilon\n",
    "        #  and computing Z= mu(x) + epsilon * sigma(x)\n",
    "        #  for predicted mu(x) and sigma(x)\n",
    "        #\n",
    "        # we will simplify *predictions* by making them deterministic\n",
    "        #  that is, for *predictions only* we will pretend z can be represented by\n",
    "        #  the predicted mean, i.e. Z=mu(x)\n",
    "        #\n",
    "        #  Why is this a simplification?\n",
    "        #  * once we get an assignment to Z, the generative network applies\n",
    "        #    a nonlinear layers before predicting the final softmax\n",
    "        #    this means that\n",
    "        #      E[softmax(f_\\theta(z))\\ != softmax(f_\\theta(E[z]))\n",
    "        #    where E[z] = mu(x)\n",
    "        #\n",
    "        #  The principled thing to do is to sample a few assignments (e.g. 100)\n",
    "        #   and run the generative model on each assignment\n",
    "        #   then use a probabilistic disambiguation rule (e.g. most-probable-sample,\n",
    "        #   or MBR, etc.).\n",
    "        #  Here instead we simply take the mean as a hopefully good approximation.\n",
    "\n",
    "\n",
    "        # 2.c Marginalise alignments: \\sum_a P(a|x) P(Y|x,a)\n",
    "\n",
    "        # Here comes a rather fancy matrix multiplication.\n",
    "        # Note that tf.matmul is defined to do a matrix multiplication\n",
    "        # [N, M] @ [M, Vy] for each item in the first dimension B.\n",
    "        # So in the final result we have B matrices [N, Vy], i.e. [B, N, Vy].\n",
    "        #\n",
    "        # We matrix-multiply:\n",
    "        #   pa_x       Shape: [B, N, *M*]\n",
    "        #       pa_x       Shape: [B, N, *N*M*]\n",
    "        # and\n",
    "        #   py_xa      Shape: [B, *M*, Vy]\n",
    "        #       py_xa      Shape: [B, *N*M*, Vy]\n",
    "        # to get\n",
    "        #   py_x  Shape: [B, N, Vy]\n",
    "        #\n",
    "        # Note: P(y|x) = prod_j p(y_j|x) = prod_j sum_aj p(aj|m)p(y_j|x_aj)\n",
    "        py_x = tf.matmul(pa_x, py_xa, name='3')  # Shape: [B, N, Vy]\n",
    "\n",
    "\n",
    "        # ###############################################\n",
    "        # This is the MC estimate of the (negative) ELBO (because we do minimisation here)\n",
    "        #  it includes the MC estimate of the negative log likelihood\n",
    "\n",
    "        # Sample gate value s. note weird tf construction for beta disrtb.\n",
    "        # see https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/Beta\n",
    "        # Beta = tf.contrib.distributions.Beta(a,b) # constructs distributions same shape as a (and b)\n",
    "        # s = Beta.sample() # generates a single sample for each of the distributions\n",
    "\n",
    "\n",
    "\n",
    "        euler = 0.5772156649\n",
    "        # approx = tf.add_n([tf.reciprocal(m + alpha*beta) * Beta(m * tf.reciprocal(alpha), beta) for m in range(1,10)])\n",
    "        alpha = alpha + 0.0001 # to avoid division by 0\n",
    "        beta = beta + 0.0001 # to avoid division by 0\n",
    "        approx = tf.add_n([tf.reciprocal(m + alpha*beta) * tf.exp(tf.lbeta([m*tf.reciprocal(alpha), beta])) for m in range(1,10)])    \n",
    "        first = tf.multiply(tf.div(alpha - a, alpha), -euler - tf.digamma(beta) - tf.reciprocal(beta))\n",
    "        self.first = first\n",
    "        second = tf.log(tf.multiply(alpha, beta)) + tf.lbeta([alpha, beta]) - tf.multiply(beta - 1, tf.reciprocal(beta))\n",
    "        self.second = second\n",
    "        third = tf.multiply(tf.multiply(b - 1, beta), approx)\n",
    "        self.third = third\n",
    "        kl = first + second + third\n",
    "        kl = tf.reshape(kl, tf.shape(self.y), name='KL-reshape')  # reshape back to [B, N]\n",
    "        self.kl = tf.reduce_mean(tf.reduce_sum(kl * y_mask, axis=1), axis=0)\n",
    "\n",
    "        # Our sampled S is a **deterministic** function of the random noise (u)\n",
    "        # this pushes all sources of non-determinism out of the computational graph\n",
    "        # which is very convenient\n",
    "        # In formula: s = (1-u^{1/alpha})^{1/beta}\n",
    "        u = tf.random_uniform(tf.shape(alpha), minval=0, maxval=1, dtype=tf.float32)  # [B*N]\n",
    "        s = tf.pow(tf.add(-tf.pow(u, tf.reciprocal(alpha)), 1), tf.reciprocal(beta)) # hopefully this works\n",
    "        # Read the equation in Theory 2.2 carefully. Then you will see that this is correct.\n",
    "        s = tf.reshape(s, [batch_size, longest_y], name='s_reshape')\n",
    "        s_tiled = tf.expand_dims(s, 2) # Shape: [B, N, 1]\n",
    "        s_tiled = tf.tile(s_tiled, [1, 1, self.y_vocabulary_size]) # Shape: [B, N, Vy]\n",
    "\n",
    "        # Here we marginalise over S\n",
    "        py_x = tf.multiply(s_tiled, py_x, name='s1') + tf.multiply(1-s_tiled, py_y, name='s2') # Shape [B, N, Vy]\n",
    "\n",
    "        # This calculates the accuracy, i.e. how many predictions we got right.\n",
    "        predictions = tf.argmax(py_x, axis=2)\n",
    "        acc = tf.equal(predictions, self.y)\n",
    "        acc = tf.cast(acc, tf.float32) * y_mask\n",
    "        acc_correct = tf.reduce_sum(acc)\n",
    "        acc_total = tf.reduce_sum(y_mask)\n",
    "        acc = acc_correct / acc_total\n",
    "\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=tf.reshape(self.y, [-1], name='y_reshape'),\n",
    "          logits=tf.log(tf.reshape(py_x,[batch_size * longest_y, self.y_vocabulary_size], name='lastfucker')),\n",
    "          name=\"logits\"\n",
    "        )\n",
    "        cross_entropy = tf.reshape(cross_entropy, [batch_size, longest_y], name='ce-reshape')\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy * y_mask, axis=1)\n",
    "        self.ce = tf.reduce_mean(cross_entropy, axis=0)\n",
    "\n",
    "        # The total loss is the negative MC estimate of the ELBO\n",
    "        self.loss = self.ce + self.kl\n",
    "\n",
    "        self.pa_x = pa_x\n",
    "        self.py_x = py_x\n",
    "        self.py_y = py_y\n",
    "        self.py_xa = py_xa\n",
    "        self.s = s\n",
    "        self.predictions = predictions\n",
    "        self.accuracy = acc\n",
    "        self.accuracy_correct = tf.cast(acc_correct, tf.int64)\n",
    "        self.accuracy_total = tf.cast(acc_total, tf.int64)\n",
    "\n",
    "\n",
    "    def evaluate(self, data, ref_alignments, batch_size=4):\n",
    "        \"\"\"Evaluate the model on a data set.\"\"\"\n",
    "\n",
    "        ref_align = read_naacl_alignments(ref_alignments)\n",
    "\n",
    "        ref_iterator = iter(ref_align)\n",
    "        metric = AERSufficientStatistics()\n",
    "        accuracy_correct = 0\n",
    "        accuracy_total = 0\n",
    "\n",
    "        for batch_id, batch in enumerate(iterate_minibatches(data, batch_size=batch_size)):\n",
    "            x, y = prepare_data(batch, self.x_vocabulary, self.y_vocabulary)\n",
    "            y_len = np.sum(np.sign(y), axis=1, dtype=\"int64\")\n",
    "\n",
    "            align, prob, acc_correct, acc_total = self.get_viterbi(x, y)\n",
    "            accuracy_correct += acc_correct\n",
    "            accuracy_total += acc_total\n",
    "\n",
    "            for alignment, N, (sure, probable) in zip(align, y_len, ref_iterator):\n",
    "                # the evaluation ignores NULL links, so we discard them\n",
    "                # j is 1-based in the naacl format\n",
    "                pred = set((aj, j) for j, aj in enumerate(alignment[:N], 1) if aj > 0)\n",
    "                metric.update(sure=sure, probable=probable, predicted=pred)\n",
    "\n",
    "        accuracy = accuracy_correct / float(accuracy_total)\n",
    "        return metric.aer(), accuracy\n",
    "\n",
    "\n",
    "    def get_viterbi(self, x, y):\n",
    "        \"\"\"Returns the Viterbi alignment for (x, y)\"\"\"\n",
    "\n",
    "        feed_dict = {\n",
    "          self.x: x, # English\n",
    "          self.y: y, # French\n",
    "        }\n",
    "\n",
    "        # run model on this input\n",
    "        py_xa, py_y, s, acc_correct, acc_total, alpha, beta = self.session.run(\n",
    "          [self.py_xa,\n",
    "           self.py_y,\n",
    "           self.s,\n",
    "           self.accuracy_correct, \n",
    "           self.accuracy_total,\n",
    "           self.alpha,\n",
    "           self.beta],\n",
    "           feed_dict=feed_dict)\n",
    "\n",
    "        # things to return\n",
    "        batch_size, longest_y = y.shape\n",
    "        _, longest_x = x.shape\n",
    "        alignments = np.zeros((batch_size, longest_y), dtype=\"int64\")\n",
    "        probabilities = np.zeros((batch_size, longest_y), dtype=\"float32\")\n",
    "\n",
    "        for b, sentence in enumerate(y):\n",
    "            for j, french_word in enumerate(sentence):\n",
    "                if french_word == 0:  # Padding\n",
    "                    break\n",
    "                fprev = j\n",
    "                alphaj = alpha[b,j] + 0.0001\n",
    "                betaj = beta[b,j] + 0.0001\n",
    "                # if b in range(20): print(sj)\n",
    "                # Here we use the expectation of a Kuma(alpha, beta) distr\n",
    "                # with our estimated alpha and beta:\n",
    "                # s = E[S] = beta*Gamma(1+1/alpha)Gamma(beta) / Gamma(1+1/alpha+beta)\n",
    "                sj = (betaj*sp.special.gamma(1+1/(alphaj))*sp.special.gamma(betaj)) / sp.special.gamma(1+1/(alphaj)+betaj)\n",
    "                c = int(np.random.uniform() < sj) # sample c ~ Bernouilli(sj)\n",
    "                if c == 0: # then we align\n",
    "                    probs = py_xa[b, : , y[b,j]] # y[b,j] means only the word f_j in the sentence b\n",
    "                    a_j = probs.argmax()\n",
    "                    p_j = probs[a_j]\n",
    "                if c == 1: # then we `insert` (i.e. NULL align - see NLP2 blog post)\n",
    "                    # if b in range(20): print('Null aligned')\n",
    "                    a_j = 0 # NULL align\n",
    "                    p_j = 1 # not important\n",
    "\n",
    "                alignments[b, j] = a_j\n",
    "                probabilities[b, j] = p_j\n",
    "\n",
    "\n",
    "    return alignments, probabilities, acc_correct, acc_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from T4 import NeuralIBM1Model\n",
    "from T4trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On 10k sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with B=10 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss -4043.429443 ce 67.954430 kl -4111.383789 accuracy 0.09 lr 0.001000\n",
      "Iter   200 loss -4766.895020 ce 78.843971 kl -4845.738770 accuracy 0.16 lr 0.001000\n",
      "Iter   300 loss -3705.818359 ce 53.314465 kl -3759.132812 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5774.806641 ce 98.848389 kl -5873.655273 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4424.308594 ce 69.040314 kl -4493.349121 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5059.851562 ce 79.610077 kl -5139.461426 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4603.071289 ce 66.492516 kl -4669.563965 accuracy 0.21 lr 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daan/Documents/Logic/NLP2/part3/Variational_AutoEncoders/T4.py:440: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sj = (betaj*sp.special.gamma(1+1/(alphaj))*sp.special.gamma(betaj)) / sp.special.gamma(1+1/(alphaj)+betaj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss -4038.116148 accuracy 0.19 val_aer 0.68 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4049.175049 ce 62.392345 kl -4111.567383 accuracy 0.09 lr 0.001000\n",
      "Iter   200 loss -4770.917480 ce 74.866531 kl -4845.784180 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3708.629639 ce 50.525105 kl -3759.154785 accuracy 0.23 lr 0.001000\n",
      "Iter   400 loss -5778.699707 ce 94.979614 kl -5873.679199 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4428.931641 ce 64.432449 kl -4493.364258 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5062.996582 ce 76.474510 kl -5139.471191 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4607.411133 ce 62.164143 kl -4669.575195 accuracy 0.22 lr 0.001000\n",
      "Epoch 2 loss -4175.350656 accuracy 0.20 val_aer 0.52 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4058.460938 ce 53.114754 kl -4111.575684 accuracy 0.08 lr 0.001000\n",
      "Iter   200 loss -4776.706055 ce 69.082283 kl -4845.788574 accuracy 0.14 lr 0.001000\n",
      "Iter   300 loss -3711.956299 ce 47.201378 kl -3759.157715 accuracy 0.24 lr 0.001000\n",
      "Iter   400 loss -5782.601562 ce 91.082474 kl -5873.684082 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4431.780273 ce 61.587563 kl -4493.367676 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5067.376465 ce 72.097328 kl -5139.473633 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4612.600098 ce 56.978821 kl -4669.579102 accuracy 0.22 lr 0.001000\n",
      "Epoch 3 loss -4179.196560 accuracy 0.20 val_aer 0.48 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4063.145996 ce 48.432533 kl -4111.578613 accuracy 0.12 lr 0.001000\n",
      "Iter   200 loss -4781.973145 ce 63.816914 kl -4845.790039 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3714.780518 ce 44.378506 kl -3759.158936 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5786.930176 ce 86.755653 kl -5873.686035 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4433.669922 ce 59.699017 kl -4493.369141 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5069.854980 ce 69.620743 kl -5139.475586 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4614.989746 ce 54.590424 kl -4669.580078 accuracy 0.22 lr 0.001000\n",
      "Epoch 4 loss -4182.044872 accuracy 0.20 val_aer 0.46 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4064.977539 ce 46.601963 kl -4111.579590 accuracy 0.12 lr 0.001000\n",
      "Iter   200 loss -4785.193848 ce 60.597099 kl -4845.791016 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3715.812500 ce 43.346958 kl -3759.159424 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5789.623535 ce 84.062836 kl -5873.686523 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4435.121094 ce 58.249249 kl -4493.370117 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5071.210938 ce 68.264694 kl -5139.475586 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4615.419434 ce 54.161114 kl -4669.580566 accuracy 0.22 lr 0.001000\n",
      "Epoch 5 loss -4183.610446 accuracy 0.20 val_aer 0.46 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4065.644043 ce 45.935913 kl -4111.580078 accuracy 0.12 lr 0.001000\n",
      "Iter   200 loss -4786.561523 ce 59.230019 kl -4845.791504 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3716.423828 ce 42.735661 kl -3759.159424 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5790.501465 ce 83.185631 kl -5873.687012 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4436.026367 ce 57.343647 kl -4493.370117 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5071.986816 ce 67.488907 kl -5139.475586 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4615.744141 ce 53.836891 kl -4669.581055 accuracy 0.22 lr 0.001000\n",
      "Epoch 6 loss -4184.413522 accuracy 0.20 val_aer 0.45 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4066.081787 ce 45.498756 kl -4111.580566 accuracy 0.12 lr 0.001000\n",
      "Iter   200 loss -4787.292480 ce 58.499367 kl -4845.791992 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3717.042236 ce 42.117241 kl -3759.159424 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5791.066406 ce 82.620888 kl -5873.687500 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4436.562012 ce 56.808594 kl -4493.370605 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5072.443359 ce 67.032730 kl -5139.476074 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4615.979980 ce 53.600842 kl -4669.581055 accuracy 0.21 lr 0.001000\n",
      "Epoch 7 loss -4184.893863 accuracy 0.20 val_aer 0.46 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4066.312012 ce 45.269051 kl -4111.581055 accuracy 0.10 lr 0.001000\n",
      "Iter   200 loss -4787.831543 ce 57.960258 kl -4845.791992 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3717.465820 ce 41.694305 kl -3759.160156 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5791.499512 ce 82.187813 kl -5873.687500 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4436.947266 ce 56.423481 kl -4493.370605 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5072.657227 ce 66.819290 kl -5139.476562 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4616.205078 ce 53.375782 kl -4669.581055 accuracy 0.21 lr 0.001000\n",
      "Epoch 8 loss -4185.229213 accuracy 0.20 val_aer 0.46 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4066.419922 ce 45.161243 kl -4111.581055 accuracy 0.10 lr 0.001000\n",
      "Iter   200 loss -4788.235352 ce 57.556507 kl -4845.791992 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3717.732910 ce 41.427368 kl -3759.160156 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5791.866211 ce 81.821121 kl -5873.687500 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4437.300293 ce 56.070107 kl -4493.370605 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5072.853516 ce 66.623215 kl -5139.476562 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4616.415527 ce 53.165302 kl -4669.581055 accuracy 0.21 lr 0.001000\n",
      "Epoch 9 loss -4185.494182 accuracy 0.20 val_aer 0.46 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4066.495605 ce 45.085449 kl -4111.581055 accuracy 0.12 lr 0.001000\n",
      "Iter   200 loss -4788.477539 ce 57.314625 kl -4845.791992 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3717.933594 ce 41.226627 kl -3759.160156 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5792.284668 ce 81.402695 kl -5873.687500 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4437.664551 ce 55.706551 kl -4493.371094 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5073.085449 ce 66.391113 kl -5139.476562 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4616.623047 ce 52.958748 kl -4669.582031 accuracy 0.21 lr 0.001000\n",
      "Epoch 10 loss -4185.709586 accuracy 0.20 val_aer 0.46 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # some hyper-parameters\n",
    "    # tweak them as you wish\n",
    "    batch_size=10  # on CPU, use something much smaller e.g. 1-16\n",
    "    max_length=30\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "    emb_dim = 64\n",
    "    mlp_dim = 128\n",
    "\n",
    "    # our model\n",
    "    model = NeuralIBM1Model(\n",
    "        x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "        batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "\n",
    "    # our trainer\n",
    "    trainer = NeuralIBM1Trainer(\n",
    "        model, train_e_path, train_f_path, \n",
    "        dev_e_path, dev_f_path, dev_wa,\n",
    "        test_e_path, test_f_path, test_wa,\n",
    "        num_epochs=10, batch_size=batch_size, \n",
    "        max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess,\n",
    "        max_num=10000) # small training corpus just to make testing new code easier\n",
    "\n",
    "    # now first TF needs to initialize all the variables\n",
    "    print(\"Initializing variables..\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # now we can start training!\n",
    "    print(\"Training started..\")\n",
    "    results = trainer.train()\n",
    "    dev_AERs, test_AERs, train_ELBOs, dev_ELBOs = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133e6d828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAERCAYAAABRpiGMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1261f3cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEKCAYAAABQRFHsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1261f3e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_AERs)+1), dev_AERs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "ax2 = plt.plot(range(1, len(test_AERs)+1), test_AERs, label='test-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AER')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(train_ELBOs)+1), train_ELBOs, label='training-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('ELBO')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_ELBOs)+1), dev_ELBOs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('ELBO')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On 100k sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with B=10 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss -3973.885010 ce 68.410545 kl -4042.295654 accuracy 0.09 lr 0.001000\n",
      "Iter   200 loss -4767.061035 ce 78.689514 kl -4845.750488 accuracy 0.16 lr 0.001000\n",
      "Iter   300 loss -3705.911133 ce 53.218483 kl -3759.129639 accuracy 0.25 lr 0.001000\n",
      "Iter   400 loss -5775.269043 ce 98.391670 kl -5873.660645 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4424.097656 ce 69.246826 kl -4493.344238 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5060.034668 ce 79.411919 kl -5139.446777 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4603.160156 ce 66.398285 kl -4669.558594 accuracy 0.22 lr 0.001000\n",
      "Iter   800 loss -1563.177612 ce 22.714556 kl -1585.892212 accuracy 0.30 lr 0.001000\n",
      "Iter   900 loss -4518.131348 ce 63.319038 kl -4581.450195 accuracy 0.26 lr 0.001000\n",
      "Iter  1000 loss -5448.706543 ce 72.544449 kl -5521.250977 accuracy 0.29 lr 0.001000\n",
      "Iter  1100 loss -4828.939941 ce 75.568024 kl -4904.507812 accuracy 0.14 lr 0.001000\n",
      "Iter  1200 loss -4282.563965 ce 63.949291 kl -4346.513184 accuracy 0.18 lr 0.001000\n",
      "Iter  1300 loss -1509.146729 ce 18.008293 kl -1527.155029 accuracy 0.23 lr 0.001000\n",
      "Iter  1400 loss -5607.622070 ce 89.844162 kl -5697.466309 accuracy 0.13 lr 0.001000\n",
      "Iter  1500 loss -1196.598267 ce 7.506461 kl -1204.104736 accuracy 0.49 lr 0.001000\n",
      "Iter  1600 loss -4491.736328 ce 60.362770 kl -4552.099121 accuracy 0.16 lr 0.001000\n",
      "Iter  1700 loss -4570.697754 ce 69.508720 kl -4640.206543 accuracy 0.12 lr 0.001000\n",
      "Iter  1800 loss -4614.097656 ce 55.476948 kl -4669.574707 accuracy 0.26 lr 0.001000\n",
      "Iter  1900 loss -5610.132812 ce 87.336884 kl -5697.469727 accuracy 0.16 lr 0.001000\n",
      "Iter  2000 loss -5220.762695 ce 65.547089 kl -5286.309570 accuracy 0.22 lr 0.001000\n",
      "Iter  2100 loss -5220.319824 ce 65.991821 kl -5286.311523 accuracy 0.21 lr 0.001000\n",
      "Iter  2200 loss -4080.924561 ce 60.017731 kl -4140.942383 accuracy 0.18 lr 0.001000\n",
      "Iter  2300 loss -4262.651367 ce 54.503929 kl -4317.155273 accuracy 0.21 lr 0.001000\n",
      "Iter  2400 loss -4808.802246 ce 66.353302 kl -4875.155762 accuracy 0.16 lr 0.001000\n",
      "Iter  2500 loss -2850.503174 ce 27.601562 kl -2878.104736 accuracy 0.27 lr 0.001000\n",
      "Iter  2600 loss -4174.165527 ce 54.885048 kl -4229.050781 accuracy 0.19 lr 0.001000\n",
      "Iter  2700 loss -4055.201172 ce 56.375977 kl -4111.577148 accuracy 0.13 lr 0.001000\n",
      "Iter  2800 loss -5560.957520 ce 77.778419 kl -5638.735840 accuracy 0.19 lr 0.001000\n",
      "Iter  2900 loss -6435.488281 ce 84.299583 kl -6519.788086 accuracy 0.30 lr 0.001000\n",
      "Iter  3000 loss -3909.801025 ce 54.936150 kl -3964.737061 accuracy 0.17 lr 0.001000\n",
      "Iter  3100 loss -4116.613281 ce 53.700867 kl -4170.313965 accuracy 0.17 lr 0.001000\n",
      "Iter  3200 loss -4924.209961 ce 68.421318 kl -4992.631348 accuracy 0.16 lr 0.001000\n",
      "Iter  3300 loss -3971.742188 ce 51.729889 kl -4023.472168 accuracy 0.14 lr 0.001000\n",
      "Iter  3400 loss -6898.564941 ce 91.117615 kl -6989.682617 accuracy 0.22 lr 0.001000\n",
      "Iter  3500 loss -5587.824707 ce 80.278221 kl -5668.103027 accuracy 0.15 lr 0.001000\n",
      "Iter  3600 loss -3719.055664 ce 40.102119 kl -3759.157715 accuracy 0.25 lr 0.001000\n",
      "Iter  3700 loss -1052.086548 ce 5.176507 kl -1057.263062 accuracy 0.61 lr 0.001000\n",
      "Iter  3800 loss -4779.570801 ce 66.219185 kl -4845.790039 accuracy 0.18 lr 0.001000\n",
      "Iter  3900 loss -5947.331055 ce 73.196213 kl -6020.527344 accuracy 0.25 lr 0.001000\n",
      "Iter  4000 loss -1197.291504 ce 6.814177 kl -1204.105713 accuracy 0.41 lr 0.001000\n",
      "Iter  4100 loss -4952.015625 ce 69.984421 kl -5022.000000 accuracy 0.13 lr 0.001000\n",
      "Iter  4200 loss -5420.106445 ce 71.789352 kl -5491.895996 accuracy 0.26 lr 0.001000\n",
      "Iter  4300 loss -3971.882812 ce 51.591805 kl -4023.474609 accuracy 0.17 lr 0.001000\n",
      "Iter  4400 loss -4343.260254 ce 62.003273 kl -4405.263672 accuracy 0.11 lr 0.001000\n",
      "Iter  4500 loss -3658.102295 ce 42.320198 kl -3700.422607 accuracy 0.21 lr 0.001000\n",
      "Iter  4600 loss -4948.760742 ce 73.240234 kl -5022.000977 accuracy 0.12 lr 0.001000\n",
      "Iter  4700 loss -1285.730469 ce 6.480659 kl -1292.211182 accuracy 0.45 lr 0.001000\n",
      "Iter  4800 loss -4526.066406 ce 55.408459 kl -4581.475098 accuracy 0.22 lr 0.001000\n",
      "Iter  4900 loss -3618.916504 ce 52.137337 kl -3671.053955 accuracy 0.10 lr 0.001000\n",
      "Iter  5000 loss -4905.853027 ce 57.409996 kl -4963.263184 accuracy 0.18 lr 0.001000\n",
      "Iter  5100 loss -1167.583740 ce 7.153524 kl -1174.737305 accuracy 0.47 lr 0.001000\n",
      "Iter  5200 loss -2326.410889 ce 23.063709 kl -2349.474609 accuracy 0.20 lr 0.001000\n",
      "Iter  5300 loss -4780.751953 ce 65.039139 kl -4845.791016 accuracy 0.13 lr 0.001000\n",
      "Iter  5400 loss -4678.637207 ce 49.679482 kl -4728.316895 accuracy 0.25 lr 0.001000\n",
      "Iter  5500 loss -7533.114258 ce 102.677017 kl -7635.791504 accuracy 0.19 lr 0.001000\n",
      "Iter  5600 loss -2237.308838 ce 24.060385 kl -2261.369141 accuracy 0.14 lr 0.001000\n",
      "Iter  5700 loss -4254.023926 ce 63.135155 kl -4317.159180 accuracy 0.23 lr 0.001000\n",
      "Iter  5800 loss -4381.488770 ce 53.144238 kl -4434.632812 accuracy 0.15 lr 0.001000\n",
      "Iter  5900 loss -4323.875000 ce 52.021057 kl -4375.895996 accuracy 0.21 lr 0.001000\n",
      "Iter  6000 loss -4492.357910 ce 59.748669 kl -4552.106445 accuracy 0.18 lr 0.001000\n",
      "Iter  6100 loss -4260.236816 ce 56.922462 kl -4317.159180 accuracy 0.16 lr 0.001000\n",
      "Iter  6200 loss -963.825928 ce 5.332374 kl -969.158325 accuracy 0.45 lr 0.001000\n",
      "Iter  6300 loss -3314.537842 ce 33.463306 kl -3348.001221 accuracy 0.28 lr 0.001000\n",
      "Iter  6400 loss -3566.098389 ce 46.218838 kl -3612.317139 accuracy 0.13 lr 0.001000\n",
      "Iter  6500 loss -3547.512695 ce 35.435760 kl -3582.948486 accuracy 0.20 lr 0.001000\n",
      "Iter  6600 loss -2473.128174 ce 23.188816 kl -2496.316895 accuracy 0.31 lr 0.001000\n",
      "Iter  6700 loss -5708.182129 ce 77.398926 kl -5785.581055 accuracy 0.13 lr 0.001000\n",
      "Iter  6800 loss -4175.712891 ce 53.340858 kl -4229.053711 accuracy 0.21 lr 0.001000\n",
      "Iter  6900 loss -4904.188965 ce 59.075817 kl -4963.264648 accuracy 0.17 lr 0.001000\n",
      "Iter  7000 loss -4865.437988 ce 68.457993 kl -4933.895996 accuracy 0.08 lr 0.001000\n",
      "Iter  7100 loss -4833.214844 ce 71.313576 kl -4904.528320 accuracy 0.11 lr 0.001000\n",
      "Iter  7200 loss -3420.821289 ce 44.654503 kl -3465.475830 accuracy 0.12 lr 0.001000\n",
      "Iter  7300 loss -994.510864 ce 4.015883 kl -998.526733 accuracy 0.53 lr 0.001000\n",
      "Iter  7400 loss -4838.139648 ce 66.388779 kl -4904.528320 accuracy 0.21 lr 0.001000\n",
      "Iter  7500 loss -994.692383 ce 3.834588 kl -998.526978 accuracy 0.44 lr 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daan/Documents/Logic/NLP2/part3/Variational_AutoEncoders/T4.py:440: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  c = int(np.random.uniform() < sj) # sample c ~ Bernouilli(sj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss -3978.860076 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4065.739258 ce 45.841331 kl -4111.580566 accuracy 0.10 lr 0.001000\n",
      "Iter   200 loss -4787.428223 ce 58.363213 kl -4845.791504 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3716.596680 ce 42.562862 kl -3759.159424 accuracy 0.24 lr 0.001000\n",
      "Iter   400 loss -5790.060547 ce 83.627647 kl -5873.687988 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4435.177246 ce 58.193531 kl -4493.370605 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5071.927246 ce 67.548325 kl -5139.475586 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4617.308594 ce 52.273182 kl -4669.581543 accuracy 0.22 lr 0.001000\n",
      "Iter   800 loss -1574.175171 ce 11.720383 kl -1585.895508 accuracy 0.35 lr 0.001000\n",
      "Iter   900 loss -4525.512695 ce 55.962963 kl -4581.475586 accuracy 0.26 lr 0.001000\n",
      "Iter  1000 loss -5454.693848 ce 66.572968 kl -5521.266602 accuracy 0.29 lr 0.001000\n",
      "Iter  1100 loss -4838.590332 ce 65.937897 kl -4904.528320 accuracy 0.16 lr 0.001000\n",
      "Iter  1200 loss -4293.136719 ce 53.391823 kl -4346.528320 accuracy 0.18 lr 0.001000\n",
      "Iter  1300 loss -1513.892944 ce 13.265768 kl -1527.158691 accuracy 0.27 lr 0.001000\n",
      "Iter  1400 loss -5620.857910 ce 76.619347 kl -5697.477051 accuracy 0.13 lr 0.001000\n",
      "Iter  1500 loss -1197.762085 ce 6.343931 kl -1204.105957 accuracy 0.49 lr 0.001000\n",
      "Iter  1600 loss -4499.144531 ce 52.962517 kl -4552.106934 accuracy 0.15 lr 0.001000\n",
      "Iter  1700 loss -4576.198242 ce 64.014893 kl -4640.213379 accuracy 0.12 lr 0.001000\n",
      "Iter  1800 loss -4617.979492 ce 51.602150 kl -4669.581543 accuracy 0.26 lr 0.001000\n",
      "Iter  1900 loss -5618.748535 ce 78.728226 kl -5697.476562 accuracy 0.16 lr 0.001000\n",
      "Iter  2000 loss -5226.066895 ce 60.251404 kl -5286.318359 accuracy 0.22 lr 0.001000\n",
      "Iter  2100 loss -5224.602539 ce 61.716286 kl -5286.318848 accuracy 0.21 lr 0.001000\n",
      "Iter  2200 loss -4086.389893 ce 54.559792 kl -4140.949707 accuracy 0.18 lr 0.001000\n",
      "Iter  2300 loss -4269.144043 ce 48.016743 kl -4317.160645 accuracy 0.21 lr 0.001000\n",
      "Iter  2400 loss -4813.641113 ce 61.519600 kl -4875.160645 accuracy 0.16 lr 0.001000\n",
      "Iter  2500 loss -2852.043213 ce 26.063726 kl -2878.106934 accuracy 0.27 lr 0.001000\n",
      "Iter  2600 loss -4175.449707 ce 53.604778 kl -4229.054688 accuracy 0.19 lr 0.001000\n",
      "Iter  2700 loss -4057.020020 ce 54.560936 kl -4111.581055 accuracy 0.13 lr 0.001000\n",
      "Iter  2800 loss -5565.653809 ce 73.085968 kl -5638.739746 accuracy 0.19 lr 0.001000\n",
      "Iter  2900 loss -6439.750977 ce 80.042091 kl -6519.792969 accuracy 0.30 lr 0.001000\n",
      "Iter  3000 loss -3914.339844 ce 50.399109 kl -3964.739014 accuracy 0.17 lr 0.001000\n",
      "Iter  3100 loss -4120.249512 ce 50.068523 kl -4170.317871 accuracy 0.17 lr 0.001000\n",
      "Iter  3200 loss -4927.183594 ce 65.451218 kl -4992.634766 accuracy 0.16 lr 0.001000\n",
      "Iter  3300 loss -3971.928955 ce 51.547070 kl -4023.476074 accuracy 0.14 lr 0.001000\n",
      "Iter  3400 loss -6902.640137 ce 87.048431 kl -6989.688477 accuracy 0.22 lr 0.001000\n",
      "Iter  3500 loss -5591.940430 ce 76.167969 kl -5668.108398 accuracy 0.15 lr 0.001000\n",
      "Iter  3600 loss -3719.760010 ce 39.399754 kl -3759.159668 accuracy 0.25 lr 0.001000\n",
      "Iter  3700 loss -1052.045776 ce 5.218189 kl -1057.263916 accuracy 0.61 lr 0.001000\n",
      "Iter  3800 loss -4782.924316 ce 62.867847 kl -4845.791992 accuracy 0.18 lr 0.001000\n",
      "Iter  3900 loss -5949.052246 ce 71.477692 kl -6020.529785 accuracy 0.25 lr 0.001000\n",
      "Iter  4000 loss -1197.336182 ce 6.769903 kl -1204.106079 accuracy 0.39 lr 0.001000\n",
      "Iter  4100 loss -4955.996582 ce 66.006317 kl -5022.002930 accuracy 0.13 lr 0.001000\n",
      "Iter  4200 loss -5422.510254 ce 69.387733 kl -5491.897949 accuracy 0.26 lr 0.001000\n",
      "Iter  4300 loss -3973.065430 ce 50.410332 kl -4023.475830 accuracy 0.17 lr 0.001000\n",
      "Iter  4400 loss -4344.865723 ce 60.399750 kl -4405.265625 accuracy 0.11 lr 0.001000\n",
      "Iter  4500 loss -3659.493408 ce 40.929970 kl -3700.423340 accuracy 0.21 lr 0.001000\n",
      "Iter  4600 loss -4950.712891 ce 71.290268 kl -5022.002930 accuracy 0.12 lr 0.001000\n",
      "Iter  4700 loss -1285.718628 ce 6.492659 kl -1292.211304 accuracy 0.45 lr 0.001000\n",
      "Iter  4800 loss -4526.405762 ce 55.070671 kl -4581.476562 accuracy 0.22 lr 0.001000\n",
      "Iter  4900 loss -3619.913574 ce 51.140991 kl -3671.054688 accuracy 0.10 lr 0.001000\n",
      "Iter  5000 loss -4906.350586 ce 56.915611 kl -4963.266113 accuracy 0.19 lr 0.001000\n",
      "Iter  5100 loss -1167.743652 ce 6.993926 kl -1174.737549 accuracy 0.47 lr 0.001000\n",
      "Iter  5200 loss -2327.175293 ce 22.299871 kl -2349.475098 accuracy 0.20 lr 0.001000\n",
      "Iter  5300 loss -4781.595215 ce 64.196609 kl -4845.791992 accuracy 0.15 lr 0.001000\n",
      "Iter  5400 loss -4679.032715 ce 49.286102 kl -4728.318848 accuracy 0.25 lr 0.001000\n",
      "Iter  5500 loss -7535.208984 ce 100.584923 kl -7635.793945 accuracy 0.19 lr 0.001000\n",
      "Iter  5600 loss -2238.039795 ce 23.330055 kl -2261.369873 accuracy 0.17 lr 0.001000\n",
      "Iter  5700 loss -4253.367676 ce 63.792480 kl -4317.160156 accuracy 0.23 lr 0.001000\n",
      "Iter  5800 loss -4381.573730 ce 53.060223 kl -4434.633789 accuracy 0.15 lr 0.001000\n",
      "Iter  5900 loss -4324.790039 ce 51.107464 kl -4375.897461 accuracy 0.21 lr 0.001000\n",
      "Iter  6000 loss -4492.875000 ce 59.233143 kl -4552.107910 accuracy 0.18 lr 0.001000\n",
      "Iter  6100 loss -4261.432129 ce 55.728466 kl -4317.160645 accuracy 0.16 lr 0.001000\n",
      "Iter  6200 loss -964.045288 ce 5.113243 kl -969.158508 accuracy 0.45 lr 0.001000\n",
      "Iter  6300 loss -3314.836670 ce 33.165821 kl -3348.002441 accuracy 0.28 lr 0.001000\n",
      "Iter  6400 loss -3567.130127 ce 45.187847 kl -3612.317871 accuracy 0.14 lr 0.001000\n",
      "Iter  6500 loss -3547.669922 ce 35.279747 kl -3582.949707 accuracy 0.20 lr 0.001000\n",
      "Iter  6600 loss -2473.054199 ce 23.262863 kl -2496.317139 accuracy 0.31 lr 0.001000\n",
      "Iter  6700 loss -5709.589355 ce 75.993820 kl -5785.583008 accuracy 0.14 lr 0.001000\n",
      "Iter  6800 loss -4175.886719 ce 53.168568 kl -4229.055176 accuracy 0.21 lr 0.001000\n",
      "Iter  6900 loss -4905.255371 ce 58.010426 kl -4963.265625 accuracy 0.17 lr 0.001000\n",
      "Iter  7000 loss -4866.086426 ce 67.811325 kl -4933.897949 accuracy 0.08 lr 0.001000\n",
      "Iter  7100 loss -4833.531250 ce 70.997353 kl -4904.528809 accuracy 0.11 lr 0.001000\n",
      "Iter  7200 loss -3421.592041 ce 43.883675 kl -3465.475830 accuracy 0.12 lr 0.001000\n",
      "Iter  7300 loss -994.304871 ce 4.222092 kl -998.526978 accuracy 0.53 lr 0.001000\n",
      "Iter  7400 loss -4838.789062 ce 65.739571 kl -4904.528809 accuracy 0.21 lr 0.001000\n",
      "Iter  7500 loss -994.689453 ce 3.837613 kl -998.527039 accuracy 0.44 lr 0.001000\n",
      "Epoch 2 loss -4027.577970 accuracy 0.20 val_aer 0.43 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4066.246582 ce 45.334423 kl -4111.581055 accuracy 0.12 lr 0.001000\n",
      "Iter   200 loss -4787.863770 ce 57.927990 kl -4845.791992 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3716.717041 ce 42.443153 kl -3759.160156 accuracy 0.24 lr 0.001000\n",
      "Iter   400 loss -5791.264160 ce 82.423683 kl -5873.687988 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4435.381836 ce 57.989380 kl -4493.371094 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5072.344238 ce 67.132309 kl -5139.476562 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4617.570312 ce 52.011139 kl -4669.581543 accuracy 0.22 lr 0.001000\n",
      "Iter   800 loss -1574.162964 ce 11.732786 kl -1585.895752 accuracy 0.35 lr 0.001000\n",
      "Iter   900 loss -4525.821289 ce 55.654968 kl -4581.476074 accuracy 0.26 lr 0.001000\n",
      "Iter  1000 loss -5455.340820 ce 65.926010 kl -5521.266602 accuracy 0.29 lr 0.001000\n",
      "Iter  1100 loss -4838.558594 ce 65.970779 kl -4904.529297 accuracy 0.16 lr 0.001000\n",
      "Iter  1200 loss -4293.182617 ce 53.346020 kl -4346.528809 accuracy 0.18 lr 0.001000\n",
      "Iter  1300 loss -1514.197388 ce 12.961485 kl -1527.158813 accuracy 0.27 lr 0.001000\n",
      "Iter  1400 loss -5621.050293 ce 76.426979 kl -5697.477051 accuracy 0.12 lr 0.001000\n",
      "Iter  1500 loss -1197.809448 ce 6.296518 kl -1204.105957 accuracy 0.49 lr 0.001000\n",
      "Iter  1600 loss -4499.511230 ce 52.597404 kl -4552.108398 accuracy 0.15 lr 0.001000\n",
      "Iter  1700 loss -4576.393555 ce 63.819843 kl -4640.213379 accuracy 0.12 lr 0.001000\n",
      "Iter  1800 loss -4618.049805 ce 51.531471 kl -4669.581055 accuracy 0.26 lr 0.001000\n",
      "Iter  1900 loss -5620.438965 ce 77.038071 kl -5697.477051 accuracy 0.16 lr 0.001000\n",
      "Iter  2000 loss -5226.257324 ce 60.062195 kl -5286.319336 accuracy 0.22 lr 0.001000\n",
      "Iter  2100 loss -5225.510742 ce 60.808643 kl -5286.319336 accuracy 0.21 lr 0.001000\n",
      "Iter  2200 loss -4086.705078 ce 54.245129 kl -4140.950195 accuracy 0.18 lr 0.001000\n",
      "Iter  2300 loss -4269.451660 ce 47.708454 kl -4317.160156 accuracy 0.21 lr 0.001000\n",
      "Iter  2400 loss -4813.774902 ce 61.386055 kl -4875.161133 accuracy 0.16 lr 0.001000\n",
      "Iter  2500 loss -2852.042969 ce 26.063910 kl -2878.106934 accuracy 0.27 lr 0.001000\n",
      "Iter  2600 loss -4175.536133 ce 53.518738 kl -4229.054688 accuracy 0.19 lr 0.001000\n",
      "Iter  2700 loss -4056.992676 ce 54.588451 kl -4111.581055 accuracy 0.15 lr 0.001000\n",
      "Iter  2800 loss -5566.247070 ce 72.493034 kl -5638.740234 accuracy 0.19 lr 0.001000\n",
      "Iter  2900 loss -6440.916992 ce 78.876724 kl -6519.793945 accuracy 0.30 lr 0.001000\n",
      "Iter  3000 loss -3914.631104 ce 50.108467 kl -3964.739502 accuracy 0.17 lr 0.001000\n",
      "Iter  3100 loss -4120.850586 ce 49.467125 kl -4170.317871 accuracy 0.17 lr 0.001000\n",
      "Iter  3200 loss -4927.493652 ce 65.140564 kl -4992.634277 accuracy 0.16 lr 0.001000\n",
      "Iter  3300 loss -3971.944824 ce 51.531239 kl -4023.476074 accuracy 0.14 lr 0.001000\n",
      "Iter  3400 loss -6903.854004 ce 85.834541 kl -6989.688477 accuracy 0.22 lr 0.001000\n",
      "Iter  3500 loss -5592.393066 ce 75.715393 kl -5668.108398 accuracy 0.15 lr 0.001000\n",
      "Iter  3600 loss -3719.837402 ce 39.322746 kl -3759.160156 accuracy 0.25 lr 0.001000\n",
      "Iter  3700 loss -1052.049316 ce 5.214357 kl -1057.263672 accuracy 0.61 lr 0.001000\n",
      "Iter  3800 loss -4783.130859 ce 62.661083 kl -4845.791992 accuracy 0.18 lr 0.001000\n",
      "Iter  3900 loss -5949.582031 ce 70.947762 kl -6020.529785 accuracy 0.25 lr 0.001000\n",
      "Iter  4000 loss -1197.345093 ce 6.760702 kl -1204.105835 accuracy 0.39 lr 0.001000\n",
      "Iter  4100 loss -4956.315918 ce 65.687088 kl -5022.002930 accuracy 0.13 lr 0.001000\n",
      "Iter  4200 loss -5422.936035 ce 68.962105 kl -5491.897949 accuracy 0.26 lr 0.001000\n",
      "Iter  4300 loss -3973.141357 ce 50.334396 kl -4023.475830 accuracy 0.17 lr 0.001000\n",
      "Iter  4400 loss -4345.474121 ce 59.791725 kl -4405.265625 accuracy 0.11 lr 0.001000\n",
      "Iter  4500 loss -3659.608154 ce 40.815235 kl -3700.423340 accuracy 0.21 lr 0.001000\n",
      "Iter  4600 loss -4950.731445 ce 71.271683 kl -5022.002930 accuracy 0.10 lr 0.001000\n",
      "Iter  4700 loss -1285.741943 ce 6.469346 kl -1292.211304 accuracy 0.45 lr 0.001000\n",
      "Iter  4800 loss -4526.458984 ce 55.016979 kl -4581.476074 accuracy 0.22 lr 0.001000\n",
      "Iter  4900 loss -3620.041992 ce 51.012623 kl -3671.054688 accuracy 0.10 lr 0.001000\n",
      "Iter  5000 loss -4906.454590 ce 56.811817 kl -4963.266602 accuracy 0.19 lr 0.001000\n",
      "Iter  5100 loss -1167.813477 ce 6.924048 kl -1174.737549 accuracy 0.47 lr 0.001000\n",
      "Iter  5200 loss -2327.291504 ce 22.183500 kl -2349.475098 accuracy 0.20 lr 0.001000\n",
      "Iter  5300 loss -4782.071777 ce 63.720192 kl -4845.791992 accuracy 0.15 lr 0.001000\n",
      "Iter  5400 loss -4679.073242 ce 49.245998 kl -4728.319336 accuracy 0.25 lr 0.001000\n",
      "Iter  5500 loss -7537.138672 ce 98.655228 kl -7635.793945 accuracy 0.19 lr 0.001000\n",
      "Iter  5600 loss -2238.302246 ce 23.067535 kl -2261.369873 accuracy 0.17 lr 0.001000\n",
      "Iter  5700 loss -4253.305176 ce 63.855488 kl -4317.160645 accuracy 0.23 lr 0.001000\n",
      "Iter  5800 loss -4381.615723 ce 53.018776 kl -4434.634277 accuracy 0.15 lr 0.001000\n",
      "Iter  5900 loss -4324.870605 ce 51.026943 kl -4375.897461 accuracy 0.21 lr 0.001000\n",
      "Iter  6000 loss -4493.145996 ce 58.962444 kl -4552.108398 accuracy 0.18 lr 0.001000\n",
      "Iter  6100 loss -4261.591309 ce 55.569359 kl -4317.160645 accuracy 0.16 lr 0.001000\n",
      "Iter  6200 loss -964.099854 ce 5.058646 kl -969.158508 accuracy 0.45 lr 0.001000\n",
      "Iter  6300 loss -3314.828125 ce 33.173923 kl -3348.001953 accuracy 0.28 lr 0.001000\n",
      "Iter  6400 loss -3567.525635 ce 44.792355 kl -3612.317871 accuracy 0.14 lr 0.001000\n",
      "Iter  6500 loss -3547.745605 ce 35.203667 kl -3582.949219 accuracy 0.20 lr 0.001000\n",
      "Iter  6600 loss -2473.032715 ce 23.284672 kl -2496.317383 accuracy 0.31 lr 0.001000\n",
      "Iter  6700 loss -5709.890625 ce 75.691513 kl -5785.582031 accuracy 0.14 lr 0.001000\n",
      "Iter  6800 loss -4176.052246 ce 53.002911 kl -4229.055176 accuracy 0.21 lr 0.001000\n",
      "Iter  6900 loss -4905.569336 ce 57.697441 kl -4963.266602 accuracy 0.17 lr 0.001000\n",
      "Iter  7000 loss -4866.965820 ce 66.932159 kl -4933.897949 accuracy 0.08 lr 0.001000\n",
      "Iter  7100 loss -4833.699707 ce 70.829208 kl -4904.528809 accuracy 0.11 lr 0.001000\n",
      "Iter  7200 loss -3421.988770 ce 43.487110 kl -3465.475830 accuracy 0.12 lr 0.001000\n",
      "Iter  7300 loss -994.262634 ce 4.264369 kl -998.526978 accuracy 0.53 lr 0.001000\n",
      "Iter  7400 loss -4838.928711 ce 65.600204 kl -4904.528809 accuracy 0.21 lr 0.001000\n",
      "Iter  7500 loss -994.685608 ce 3.841253 kl -998.526855 accuracy 0.44 lr 0.001000\n",
      "Epoch 3 loss -4027.864043 accuracy 0.20 val_aer 0.44 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss -4066.309570 ce 45.271431 kl -4111.581055 accuracy 0.12 lr 0.001000\n",
      "Iter   200 loss -4788.163086 ce 57.629639 kl -4845.792480 accuracy 0.15 lr 0.001000\n",
      "Iter   300 loss -3716.822754 ce 42.337414 kl -3759.160156 accuracy 0.24 lr 0.001000\n",
      "Iter   400 loss -5791.671875 ce 82.015999 kl -5873.687988 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss -4435.647461 ce 57.723206 kl -4493.370605 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss -5072.416016 ce 67.060715 kl -5139.476562 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss -4617.646484 ce 51.934834 kl -4669.581543 accuracy 0.22 lr 0.001000\n",
      "Iter   800 loss -1574.161987 ce 11.733760 kl -1585.895752 accuracy 0.35 lr 0.001000\n",
      "Iter   900 loss -4525.899414 ce 55.577320 kl -4581.476562 accuracy 0.26 lr 0.001000\n",
      "Iter  1000 loss -5455.629395 ce 65.636993 kl -5521.266602 accuracy 0.29 lr 0.001000\n",
      "Iter  1100 loss -4838.944824 ce 65.584274 kl -4904.529297 accuracy 0.16 lr 0.001000\n",
      "Iter  1200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Epoch 4 loss    nan accuracy 0.03 val_aer 1.00 val_acc 0.00\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter   200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter   300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter   400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter   500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter   600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter   700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter   800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter   900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  1900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  2900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  3900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  4900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  5900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6600 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6700 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6800 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  6900 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7000 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7100 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7200 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7300 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7400 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Iter  7500 loss    nan ce nan kl nan accuracy 0.00 lr 0.001000\n",
      "Epoch 5 loss    nan accuracy 0.00 val_aer 1.00 val_acc 0.00\n",
      "Computing training-set likelihood\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # some hyper-parameters\n",
    "    # tweak them as you wish\n",
    "    batch_size=10  # on CPU, use something much smaller e.g. 1-16\n",
    "    max_length=30\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "    emb_dim = 64\n",
    "    mlp_dim = 128\n",
    "\n",
    "    # our model\n",
    "    model = NeuralIBM1Model(\n",
    "        x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "        batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "\n",
    "    # our trainer\n",
    "    trainer = NeuralIBM1Trainer(\n",
    "        model, train_e_path, train_f_path, \n",
    "        dev_e_path, dev_f_path, dev_wa,\n",
    "        test_e_path, test_f_path, test_wa,\n",
    "        num_epochs=10, batch_size=batch_size, \n",
    "        max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess,\n",
    "        max_num=100000) # small training corpus just to make testing new code easier\n",
    "\n",
    "    # now first TF needs to initialize all the variables\n",
    "    print(\"Initializing variables..\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # now we can start training!\n",
    "    print(\"Training started..\")\n",
    "    results = trainer.train()\n",
    "    dev_AERs, test_AERs, train_ELBOs, dev_ELBOs = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_AERs)+1), dev_AERs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "ax2 = plt.plot(range(1, len(test_AERs)+1), test_AERs, label='test-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AER')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(train_ELBOs)+1), train_ELBOs, label='training-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('ELBO')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_ELBOs)+1), dev_ELBOs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('ELBO')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
